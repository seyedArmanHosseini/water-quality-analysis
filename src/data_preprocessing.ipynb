{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEe4UJRocx-Q"
      },
      "outputs": [],
      "source": [
        "# Importing all the necessary libraries for data processing, visualization, modeling, and evaluation\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Visualization\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Preprocessing\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Modeling\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor, plot_importance\n",
        "\n",
        "# Evaluation\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Utilities\n",
        "import joblib\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load Dataset from a Specified Path\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = pd.read_csv('/kaggle/input/water-quality-index-wqi/Results_MADE.csv')\n",
        "\n",
        "print(\"Displaying first 5 rows of the dataset:\")\n",
        "display(data.head())\n",
        "\n",
        "print(\"\\nDataset Information:\")\n",
        "print(data.info())\n",
        "\n",
        "print(\"\\nDescriptive Statistics:\")\n",
        "display(data.describe())\n",
        "\n",
        "print(\"\\nMissing Values Count per Column:\")\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# ==============================================\n",
        "# 2. Handling Missing Values\n",
        "# ==============================================\n",
        "\n",
        "print(\"\\nFilling missing values with column mean...\")\n",
        "\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "data_imputed = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)\n",
        "\n",
        "# ==============================================\n",
        "# 3. Removing Outliers\n",
        "# ==============================================\n",
        "\n",
        "def remove_outliers(df, column):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower = Q1 - 1.5 * IQR\n",
        "    upper = Q3 + 1.5 * IQR\n",
        "    return df[(df[column] >= lower) & (df[column] <= upper)]\n",
        "\n",
        "print(\"\\nRemoving outliers from numerical columns...\")\n",
        "\n",
        "num_cols = data_imputed.select_dtypes(include=['float64', 'int64']).columns\n",
        "before_len = len(data_imputed)\n",
        "\n",
        "for col in num_cols:\n",
        "    data_imputed = remove_outliers(data_imputed, col)\n",
        "\n",
        "after_len = len(data_imputed)\n",
        "print(f\"Records before: {before_len} / after: {after_len}\")\n",
        "\n",
        "# ==============================================\n",
        "# 4. Normalizing Features\n",
        "# ==============================================\n",
        "\n",
        "print(\"\\nNormalizing features using StandardScaler...\")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "data_scaled = pd.DataFrame(scaler.fit_transform(data_imputed), columns=data_imputed.columns)\n",
        "\n",
        "# ==============================================\n",
        "# 5. Splitting Features and Target (WQI)\n",
        "# ==============================================\n",
        "\n",
        "X = data_scaled.drop('WQI', axis=1)\n",
        "y = data_scaled['WQI']\n",
        "\n",
        "# ==============================================\n",
        "# 6. Train-Test Split\n",
        "# ==============================================\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"\\nData is ready for modeling:\")\n",
        "print(f\"Train samples: {X_train.shape[0]}\")\n",
        "print(f\"Test samples : {X_test.shape[0]}\")"
      ],
      "metadata": {
        "id": "kxANWEaYdDYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# 1. Load the dataset\n",
        "# ==============================\n",
        "data = pd.read_csv('/kaggle/input/water-quality-index-wqi/Results_MADE.csv')  # Change path accordingly\n",
        "\n",
        "# ==============================\n",
        "# 2. Handle missing values\n",
        "# ==============================\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "data_imputed = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)\n",
        "\n",
        "# ==============================\n",
        "# 3. Remove outliers\n",
        "# ==============================\n",
        "def remove_outliers(df, column):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
        "\n",
        "numeric_columns = data_imputed.select_dtypes(include=['float64', 'int64']).columns\n",
        "for col in numeric_columns:\n",
        "    data_imputed = remove_outliers(data_imputed, col)\n"
      ],
      "metadata": {
        "id": "gL5dwO4hdQkh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}